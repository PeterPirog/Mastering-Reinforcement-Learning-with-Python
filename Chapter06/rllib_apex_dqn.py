import pprint
from ray import tune
from ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG
from ray.rllib.agents.dqn.apex import ApexTrainer

if __name__ == '__main__':
    config = APEX_DEFAULT_CONFIG.copy()
    print(config['optimizer']['debug'])

    pp = pprint.PrettyPrinter(indent=4)

    config['env'] = "CartPole-v0"
    config['num_gpus'] = 0
    config['num_workers'] = 50
    config['evaluation_num_workers'] = 10
    config['evaluation_interval'] = 1
    config['learning_starts'] = 5000
    pp.pprint(config)
    tune.run(ApexTrainer, config=config)


    config = {'_disable_action_flattening': False,
              '_disable_execution_plan_api': False,
              '_disable_preprocessor_api': False,
              '_fake_gpus': False,
              '_tf_policy_handles_more_than_one_loss': False,
              'action_space': None,
              'actions_in_input_normalized': False,
              'adam_epsilon': 1e-08,
              'always_attach_evaluation_results': False,
              'batch_mode': 'truncate_episodes',
              'before_learn_on_batch': None,
              'buffer_size': 2000000,
              # 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,
              'clip_actions': False,
              'clip_rewards': None,
              'collect_metrics_timeout': -1,
              'compress_observations': False,
              'create_env_on_driver': False,
              'custom_eval_function': None,
              'custom_resources_per_worker': {},
              'disable_env_checking': False,
              'double_q': True,
              'dueling': True,
              'eager_max_retraces': 20,
              'eager_tracing': False,
              'env': 'CartPole-v0',
              'env_config': {},
              'env_task_fn': None,
              'evaluation_config': {'explore': False},
              'evaluation_duration': 10,
              'evaluation_duration_unit': 'episodes',
              'evaluation_interval': 1,
              'evaluation_num_episodes': -1,
              'evaluation_num_workers': 10,
              'evaluation_parallel_to_training': False,
              'exploration_config': {'epsilon_timesteps': 10000,
                                     'final_epsilon': 0.02,
                                     'initial_epsilon': 1.0,
                                     'type': 'PerWorkerEpsilonGreedy'},
              'explore': True,
              'extra_python_environs_for_driver': {},
              'extra_python_environs_for_worker': {},
              'fake_sampler': False,
              'final_prioritized_replay_beta': 0.4,
              'framework': 'tf',
              'gamma': 0.99,
              'grad_clip': 40,
              'hiddens': [256],
              'horizon': None,
              'ignore_worker_failures': False,
              'in_evaluation': False,
              'input': 'sampler',
              'input_config': {},
              'input_evaluation': ['is', 'wis'],
              'keep_per_episode_custom_metrics': False,
              'learning_starts': 5000,
              'local_tf_session_args': {'inter_op_parallelism_threads': 8,
                                        'intra_op_parallelism_threads': 8},
              'log_level': 'WARN',
              'log_sys_usage': True,
              'logger_config': None,
              'lr': 0.0005,
              'lr_schedule': None,
              'metrics_episode_collection_timeout_s': 180,
              'metrics_num_episodes_for_smoothing': 100,
              'metrics_smoothing_episodes': -1,
              'min_iter_time_s': -1,
              'min_sample_timesteps_per_reporting': None,
              'min_time_s_per_reporting': 30,
              'min_train_timesteps_per_reporting': None,
              'model': {'_disable_action_flattening': False,
                        '_disable_preprocessor_api': False,
                        '_time_major': False,
                        '_use_default_native_models': False,
                        'attention_dim': 64,
                        'attention_head_dim': 32,
                        'attention_init_gru_gate_bias': 2.0,
                        'attention_memory_inference': 50,
                        'attention_memory_training': 50,
                        'attention_num_heads': 1,
                        'attention_num_transformer_units': 1,
                        'attention_position_wise_mlp_dim': 32,
                        'attention_use_n_prev_actions': 0,
                        'attention_use_n_prev_rewards': 0,
                        'conv_activation': 'relu',
                        'conv_filters': None,
                        'custom_action_dist': None,
                        'custom_model': None,
                        'custom_model_config': {},
                        'custom_preprocessor': None,
                        'dim': 84,
                        'fcnet_activation': 'tanh',
                        'fcnet_hiddens': [256, 256],
                        'framestack': True,
                        'free_log_std': False,
                        'grayscale': False,
                        'lstm_cell_size': 256,
                        'lstm_use_prev_action': False,
                        'lstm_use_prev_action_reward': -1,
                        'lstm_use_prev_reward': False,
                        'max_seq_len': 20,
                        'no_final_linear': False,
                        'post_fcnet_activation': 'relu',
                        'post_fcnet_hiddens': [],
                        'use_attention': False,
                        'use_lstm': False,
                        'vf_share_layers': True,
                        'zero_mean': True},
              'monitor': -1,
              'multiagent': {'count_steps_by': 'env_steps',
                             'observation_fn': None,
                             'policies': {},
                             'policies_to_train': None,
                             'policy_map_cache': None,
                             'policy_map_capacity': 100,
                             'policy_mapping_fn': None,
                             'replay_mode': 'independent'},
              'n_step': 3,
              'no_done_at_end': False,
              'noisy': False,
              'normalize_actions': True,
              'num_atoms': 1,
              'num_cpus_for_driver': 1,
              'num_cpus_per_worker': 1,
              'num_envs_per_worker': 1,
              'num_gpus': 0,
              'num_gpus_per_worker': 0,
              'num_workers': 50,
              'observation_filter': 'NoFilter',
              'observation_space': None,
              'optimizer': {'debug': False,
                            'max_weight_sync_delay': 400,
                            'num_replay_buffer_shards': 4},
              'output': None,
              'output_compress_columns': ['obs', 'new_obs'],
              'output_config': {},
              'output_max_file_size': 67108864,
              'placement_strategy': 'PACK',
              'postprocess_inputs': False,
              'preprocessor_pref': 'deepmind',
              'prioritized_replay': True,
              'prioritized_replay_alpha': 0.6,
              'prioritized_replay_beta': 0.4,
              'prioritized_replay_beta_annealing_timesteps': 20000,
              'prioritized_replay_eps': 1e-06,
              'record_env': False,
              'remote_env_batch_wait_ms': 0,
              'remote_worker_envs': False,
              'render_env': False,
              'replay_buffer_config': None,
              'replay_buffer_shards_colocated_with_driver': True,
              'replay_sequence_length': 1,
              'rollout_fragment_length': 50,
              'sample_async': False,
              # 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,
              'seed': None,
              'shuffle_buffer_size': 0,
              'sigma0': 0.5,
              'simple_optimizer': -1,
              'soft_horizon': False,
              'store_buffer_in_checkpoints': False,
              'synchronize_filters': True,
              'target_network_update_freq': 500000,
              'tf_session_args': {'allow_soft_placement': True,
                                  'device_count': {'CPU': 1},
                                  'gpu_options': {'allow_growth': True},
                                  'inter_op_parallelism_threads': 2,
                                  'intra_op_parallelism_threads': 2,
                                  'log_device_placement': False},
              'timesteps_per_iteration': 25000,
              'train_batch_size': 512,
              'training_intensity': None,
              'v_max': 10.0,
              'v_min': -10.0,
              'worker_side_prioritization': True}
